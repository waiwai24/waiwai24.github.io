<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title># 模型隐私攻击,模型隐私攻击</title>
    <link rel="stylesheet" href="/assets/css/notes.min.css">
</head>
<body>
    <div class="main-content-wrapper">
        <main class="content">
            <h1>模型隐私攻击</h1>
<h2>1.成员推理攻击</h2>
<p>攻击者的核心目标：判断一个特定的数据点是否被用来训练某个机器学习模型</p>
<p>关键漏洞：模型&quot;背书&quot;</p>
<p>攻击步骤：攻击者（通常）无法直接看到目标模型的内部参数。他们能做的是向模型发送数据并观察其输出（即“黑盒”攻击）。为了区分“成员”和“非成员”，攻击者会训练一个**“攻击模型”（Attack Model）**。这个“攻击模型”的输入不是原始数据（比如照片），而是目标模型的输出（比如预测的概率向量，也叫 posteriors）。 这个“攻击模型”的输出是一个简单的二分类：“是成员” 或 “不是成员”</p>
<p>整个过程如下：</p>
<ul>
<li>步骤 1：训练“影子模型”
<ul>
<li>收集数据： 攻击者找到一个和目标模型类似的数据集。例如，如果目标模型是识别人脸的，攻击者就去网上爬取大量人脸照片。</li>
<li>划分数据： 攻击者将自己的数据集分成两半：一半作为“影子训练集”（Shadow Training Set），另一半作为“影子测试集”（Shadow Test Set）</li>
<li>训练影子模型： 攻击者用“影子训练集”训练一个（或多个）“影子模型”。这个影子模型的结构、任务都和“目标模型”尽可能一致。</li>
</ul>
</li>
<li>步骤 2：为“攻击模型”准备训练数据，现在，攻击者扮演“上帝”的角色，因为他完全知道哪些数据是影子模型的“成员”，哪些是“非成员”。
<ul>
<li>获取“成员”数据： 攻击者将“影子训练集”（成员）喂给影子模型，收集所有的输出概率。并给这些输出打上标签：1 (是成员)。</li>
<li>获取“非成员”数据： 攻击者将“影子测试集”（非成员）喂给影子模型，收集所有的输出概率。并给这些输出打上标签：0 (不是成员)。</li>
<li>现在，攻击者有了一个全新的数据集：(模型输出A, 1), (模型输出B, 0), (模型输出C, 1) …</li>
</ul>
</li>
<li>步骤 3：训练“攻击模型”
<ul>
<li>攻击者使用上一步创建的 (模型输出, 成员标签) 数据集，训练一个简单的二元分类器（比如逻辑回归、小型神经网络）。</li>
<li>这个分类器就是最终的“攻击模型”。它的任务是：“给我一个模型的输出概率，我来告诉你这个输出是来自一个‘成员’数据还‘非成员’数据。”</li>
</ul>
</li>
<li>步骤 4：执行攻击
<ul>
<li>攻击者拿出他真正关心的数据点（例如，你的照片）。</li>
<li>他将这张照片喂给“目标模型”（那个他不拥有但可以查询的“受害者”模型）。</li>
<li>“目标模型”返回一个输出概率，例如 [0.01, 0.99]。</li>
<li>攻击者将这个 [0.01, 0.99] 喂给 他自己训练好的“攻击模型”。</li>
<li>“攻击模型”预测结果：0.85（即 85% 的概率是“成员”）。</li>
<li>攻击完成： 攻击者得出结论，你的这张照片很可能被用于训练这个目标模型。</li>
</ul>
</li>
</ul>
<h2>2.属性推理攻击</h2>
<p>这种攻击比成员推理更“贪婪”，它不仅想知道“你是否在数据集中”，更想知道“关于你的一个我不知道的秘密是什么？”</p>
<p>攻击者的目标是：已知某个数据点（例如，一个特定的用户）在训练集中，攻击者想猜测（或推断）这个数据点某个缺失的、敏感的属性（例如，用户的收入、性取向或邮政编码）。</p>
<p>属性推理攻击之所以能成功，核心漏洞是模型在学习主要任务时，无意中也学会了数据属性之间的“相关性”</p>
<p>例子来解释：</p>
<ul>
<li>目标模型（受害者）： 一个公司训练了一个模型，用于预测员工的“工作表现”（高或低）。</li>
<li>训练数据： 包含 年龄、部门、薪水 等字段。</li>
<li>假设： 在这个公司的训练数据中，薪水 和 工作表现 恰好存在某种强相关性（例如，薪水高的人，工作表现也倾向于“高”）。</li>
<li>漏洞： 模型为了在“预测工作表现”这个主要任务上做得更准，它会被迫去学习 薪水 和 工作表现 之间的这种隐藏关系。</li>
</ul>
<p>模型“记住”了这个偏见：薪水 高 -&gt; 更可能是 工作表现高。</p>
<p>整个过程如下：</p>
<ul>
<li>
<p>训练影子模型： 攻击者使用自己的（包含薪水属性的）数据集，训练一个和目标模型类似的“影子模型”。</p>
</li>
<li>
<p>准备“攻击模型”的训练集：</p>
<ul>
<li>攻击者从他的影子数据集中取出数据。</li>
</ul>
<ul>
<li>
<p>输入 (X)： 将数据喂给“影子模型”，获取其输出的概率向量（posteriors）。</p>
</li>
<li>
<p>标签 (Y)： 这条数据对应的敏感属性（例如 薪水: 高）。</p>
</li>
</ul>
</li>
<li>
<p>训练“攻击模型”：</p>
<ul>
<li>攻击者训练了一个新的分类器。</li>
<li>这个分类器的任务是：学习 “从模型输出概率” -&gt; “反推原始属性” 之间的映射关系。</li>
<li>（例如，输入 [高表现: 0.99] -&gt; 预测 薪水: 高；输入 [高表现: 0.7] -&gt; 预测 薪水: 低）</li>
</ul>
</li>
<li>
<p>执行攻击：</p>
<ul>
<li>攻击者获取 Alice 的部分数据。</li>
</ul>
<ul>
<li>
<p>他将这些数据喂给“目标模型”（受害者）。</p>
</li>
<li>
<p>“目标模型”返回一个概率向量（例如 [高表现: 0.98]）。</p>
</li>
<li>
<p>攻击者将这个 [高表现: 0.98] 喂给他训练好的“攻击模型”。</p>
</li>
<li>
<p>“攻击模型”输出预测结果：薪水: 高。</p>
</li>
<li>
<p>攻击完成。</p>
</li>
</ul>
</li>
</ul>
<h2>3.两种攻击的区别</h2>
<p><img src="/0101/AI/assets/22.%E6%A8%A1%E5%9E%8B%E9%9A%90%E7%A7%81%E6%94%BB%E5%87%BB/image-20251023105502482.png" alt="image-20251023105502482"></p>

        </main>
    </div>
</body>
</html>